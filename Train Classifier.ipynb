{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train Classifier.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kderbfw2a_lx","colab_type":"text"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/co-corporeality/tutorial-notebooks/blob/master/Train%20Classifier.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"CBV70Wt-i7tq","colab_type":"text"},"source":["In this notebook, you will learn how to train a ResNet34 classifier to recognize objects. After training the model by capturing photos from the webcam, it can be exported, run on your local machine, and connected to another application like Unity."]},{"cell_type":"markdown","metadata":{"id":"tg9nJ2umTa3n","colab_type":"text"},"source":["Mount gDrive"]},{"cell_type":"code","metadata":{"id":"jNtBFSTU_sM3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SBDwWdJvTl7-","colab_type":"text"},"source":["Install required Linux packages"]},{"cell_type":"code","metadata":{"id":"O3zC_q8HHm9k","colab_type":"code","colab":{}},"source":["!apt-get update && apt-get upgrade -y &&\\\n","    apt-get autoremove -y\n","!sudo apt install -y --no-install-recommends \\\n","      build-essential \\\n","      cpio \\\n","      curl \\\n","      git \\\n","      lsb-release \\\n","      pciutils \\\n","      python3.6 \\\n","      python3-pip \\\n","      sudo \\\n","      libusb-1.0-0 libboost-program-options1.62.0 \\\n","      libboost-thread1.62.0 libboost-filesystem1.62.0 \\\n","      libssl1.0.0 libudev1 libjson-c3 usbutils udev wget"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYJHrX_kTp_Q","colab_type":"text"},"source":["Install OpenVino"]},{"cell_type":"code","metadata":{"id":"kwV-A8ukIW2M","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","openvino_path = \"/content/drive/My Drive/openvino/l_openvino_toolkit.tgz\" \n","  \n","!tar xf \"{openvino_path}\" &&\\\n","    cd l_openvino_toolkit_p* && \\\n","    ./install_openvino_dependencies.sh &&\\\n","    sed -i 's/decline/accept/g' silent.cfg && \\\n","    ./install.sh --silent silent.cfg  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7_uBAFVyTsgW","colab_type":"text"},"source":["Install OpenVino prerequisites"]},{"cell_type":"code","metadata":{"id":"sdjXv1XfMmyv","colab_type":"code","colab":{}},"source":["!/opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGcAbWUPTzUE","colab_type":"text"},"source":["Import Python libraries"]},{"cell_type":"code","metadata":{"id":"F1VgkhiaBAW4","colab_type":"code","colab":{}},"source":["from fastai.vision import *\n","from pathlib import Path\n","import numpy as np\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpmjjEDUBIoZ","colab_type":"code","colab":{}},"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import os\n","\n","def take_photos(image_class, quality=0.8):\n","  js = Javascript('''\n","    async function takePhotos(quality, imageClass) {\n","      const div = document.createElement('div');\n","\n","      const header = document.createElement('div');\n","      const capture = document.createElement('button');\n","      const leftCounter = document.createElement('span');\n","      \n","      let toCapture = 30;\n","\n","      header.innerHTML = '<h2>Capturing images for class <b>' + imageClass + '</b></h2><br/>';\n","      capture.textContent = 'Capture';\n","      leftCounter.innerHTML = toCapture + ' left.';\n","      \n","      div.appendChild(header);\n","      div.appendChild(capture);\n","      div.appendChild(leftCounter);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","\n","      const images = document.createElement('div');\n","      images.style.display = 'block';\n","      \n","      images.style.width = '100%';\n","      images.style.height = '200px';\n","      images.style.overflow = 'scroll';\n","      images.style.backgroundColor = 'gray';\n","\n","      div.appendChild(images);\n","\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      let urls = [];\n","      const captureCanvas = document.createElement('canvas');\n","\n","      let finished = {};\n","\n","      capture.onclick = () => {\n","        captureCanvas.width = video.videoWidth;\n","        captureCanvas.height = video.videoHeight;          \n","        captureCanvas.getContext('2d').drawImage(video, 0, 0);\n","        urls.push(captureCanvas.toDataURL('image/jpeg', quality));\n","\n","        const displayCanvas = document.createElement('canvas');\n","        displayCanvas.width = video.videoWidth / 5.0;\n","        displayCanvas.height = video.videoHeight / 5.0;\n","        images.appendChild(displayCanvas);\n","        let context = displayCanvas.getContext('2d');\n","        context.scale(1.0/5.0, 1.0/5.0);\n","        context.drawImage(captureCanvas, 0, 0);\n","\n","        --toCapture;\n","        header.innerHTML = '<h2>Capturing images for class <b>' + imageClass + '</b></h2><br/>';\n","        leftCounter.innerHTML = toCapture + ' left.';\n","\n","        if (toCapture == 0) {\n","            finished.call();\n","        }\n","      };\n","\n","      await new Promise((resolve) => finished.call = resolve);\n","      \n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      \n","      return urls;\n","    }\n","    ''')\n","  display(js)\n","  datalist = eval_js('takePhotos({}, \"{}\")'.format(quality, image_class))\n","  try:\n","    os.rmdir('data')\n","  except:\n","    pass\n","  \n","  try:\n","    data_path = f'data/{image_class}'\n","    os.makedirs(data_path, exist_ok=True)\n","  except:\n","    print('Data path already exists.')\n","  \n","  i = 0\n","  filenames = []\n","  for data in datalist:\n","    binary = b64decode(data.split(',')[1])\n","    filename = f'{data_path}/photo{i}.jpg'\n","    with open(filename, 'wb') as f:\n","      f.write(binary)\n","    i += 1\n","    filenames.append(filename)\n","  \n","  return filenames"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kO018BML6IcA","colab_type":"code","colab":{}},"source":["classes = ['apple', 'kiwi']\n","\n","try:\n","  for c in classes:\n","    filenames = take_photos(c)\n","except Exception as err:\n","  print(str(err))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UB1jKJ_hlgs","colab_type":"code","colab":{}},"source":["np.random.seed(42)\n","data = ImageDataBunch.from_folder('data', valid_pct=0.2,\n","        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vr7TUQwieoD","colab_type":"code","colab":{}},"source":["data.show_batch(rows=3, figsize=(7,8))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0y0QUzYklX3","colab_type":"code","colab":{}},"source":["learn = cnn_learner(data, models.resnet34, metrics=error_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M-7pzPIgktSl","colab_type":"code","colab":{}},"source":["learn.fit_one_cycle(4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tt6wrv7hkwuM","colab_type":"code","colab":{}},"source":["interp = ClassificationInterpretation.from_learner(learn)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcE3ZSlCk2eA","colab_type":"code","colab":{}},"source":["interp.plot_confusion_matrix()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmPqhXgtk3-Y","colab_type":"code","colab":{}},"source":["interp.plot_top_losses(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K6BlVkyTT5tO","colab_type":"text"},"source":["Export the trained model to ONNX"]},{"cell_type":"code","metadata":{"id":"bP6jVBJOlD9W","colab_type":"code","colab":{}},"source":["import torch\n","model = learn.model\n","\n","dummy_input = torch.Tensor(torch.randn(1, 3, 224, 224)).to(device)\n","\n","export_model = nn.Sequential(*(list(model.children()) + [nn.Softmax(dim=1)])) \n","\n","os.mkdir('/content/drive/My Drive/openvino_export/')\n","torch.onnx.export(export_model, dummy_input, '/content/drive/My Drive/openvino_export/model.onnx', input_names=['input'], output_names=['output'], verbose=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WE7p4vmGT9qJ","colab_type":"text"},"source":["Convert the ONNX model to the OpenVino format"]},{"cell_type":"code","metadata":{"id":"G-Akvo21lKQN","colab_type":"code","colab":{}},"source":["! source /opt/intel/openvino/bin/setupvars.sh\n","! python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model /content/drive/My\\ Drive/openvino_export/model.onnx --output_dir /content/drive/My\\ Drive/openvino_export/ --mean_values \"[123.675, 116.28, 103.53]\" --scale_values \"[58.395, 57.12, 57.375]\" --reverse_input_channels"],"execution_count":0,"outputs":[]}]}